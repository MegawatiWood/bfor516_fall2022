{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this lab, we will try to predict malicious network attacks. \n",
    "There are many different types of attacks, but for this lab we\n",
    "will only try to predict normal vs malicious. This is the same\n",
    "data that we used in Lab 2.\n",
    "\n",
    "\n",
    "# Data\n",
    "The data comes from \n",
    "[KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html).\n",
    "The data contains several million network connections from a lab\n",
    "environment. The purpose of this data was to help build a network\n",
    "intrusion detector. \n",
    "\n",
    "The full dataset is very large, consisting of nearly 5 million rows. You may wish\n",
    "to use the 10% subset of the data while writing your code to speed up\n",
    "model training. Once your code is proper, you can remove the\n",
    "subset and instead use the entire dataset.\n",
    "\n",
    "Download the files `kddcup.data_10_percent.gz` and `kddcup.names` from the website.\n",
    "\n",
    "# Preprocessing\n",
    "We will clean the data in the same manner as in Lab 2. \n",
    "\n",
    "## Imports\n",
    "For this lab we will be using functions from the popular \n",
    "[scikit-learn project](https://scikit-learn.org/stable/index.html). \n",
    "Import the functions shown below. We will use these later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import tree\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "names = pd.read_csv('data/kddcup.names', header=None, delimiter=':',skiprows=1)\n",
    "\n",
    "# make column 0 into a list\n",
    "name_list = names[0].tolist()\n",
    "\n",
    "# add the last column with type\n",
    "name_list.append('type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the column names\n",
    "netattacks = pd.read_csv('data/kddcup.data_10_percent_corrected', names=name_list, header=None, index_col=None)\n",
    "\n",
    "# netattacks.head()\n",
    "# netattacks.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create labels for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netattacks['label'] = np.where(netattacks['type'] == 'normal.', 'good', 'bad')\n",
    "# netattacks['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment data\n",
    "Next, we will create train and test sets of the data. We will\n",
    "fit the model with the training set, and use the test set to evaluate the model.\n",
    "We will do a 75/25 split (75% will be training data).\n",
    "\n",
    "The `train_test_split` function in the `sklearn.model_selection` package makes this simple.\n",
    "Notice that there are two return values from this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(netattacks, test_size=0.25)\n",
    "print(\"Rows in train:\", len(train))\n",
    "print(\"Rows in test:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "We will use the training data to train the model. You can \n",
    "use all of the variables, or just a small subset of the predictors\n",
    "(in this case, just two values for prediction). \n",
    "\n",
    "The function we will use is `tree.DecisionTreeClassifier()` from\n",
    "the `sklearn` package. The full documentation on how to use\n",
    "this classifier is [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontree#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "Unfortunately, the `DecisionTreeClassifier()` in `sklearn` does not accept categorical\n",
    "variables without some data conversion. We will leave that aside \n",
    "for this lesson and use only continuous variables. For more information \n",
    "on this issue, see the [bug report](https://github.com/scikit-learn/scikit-learn/pull/12866)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "# train the model using a list of column names\n",
    "pred_vars = ['src_bytes', 'dst_bytes']\n",
    "# The value we are trying to predict is 'label'\n",
    "dt.fit(train.loc[:, pred_vars], train['label'])\n",
    "# tree.plot_tree(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is relatively fast, and should take less than a minute on most machines\n",
    "when using the 10% data. If using the full dataset, it may take longer. \n",
    "\n",
    "# Predict labels for test data\n",
    "The next step is to evaluate the performance of the model. The first step is\n",
    "running the predicted data through the model. We will receive a list of \n",
    "the predicted label for each row. You must use the same columns \n",
    "for prediction as you used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = dt.predict(test.loc[:, pred_vars])\n",
    "print(predicted[:5]) # show first five predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get look at the breakdown of each predicted label.\n",
    "The description of the code below is \n",
    "[here](https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# count test data\n",
    "test_labels_stats = Counter(test['label'])\n",
    "print(\"Labels in the test data:\", test_labels_stats)\n",
    "\n",
    "# count predicted\n",
    "predicted_labels_stats = Counter(predicted)\n",
    "print(\"Labels in the predictions:\", predicted_labels_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stats\n",
    "\n",
    "You can view the many types of model statistics on the \n",
    "[scikit-learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).\n",
    "Most of the model statistics are based on a comparison of the *predicted* label for \n",
    "each row and its *true* label in the original data.\n",
    "\n",
    "## Confusion Matrix\n",
    "First, let's see the confusion matrix. We can use the \n",
    "`sklearn.metrics` import to generate this. We specify\n",
    "the labels such that `good` is the positive class\n",
    "and `bad` is the negative class. \n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_true=test['label'], y_pred=predicted, labels=['good', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ouptut is quite ugly. We can use the \n",
    "`plot_conufsion_matrix` function to generate a \n",
    "nice-looking plot of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(dt, test.loc[:, pred_vars], test['label'], labels=['good', 'bad'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model rarely predicted traffic as `good`. When it \n",
    "did predict traffic as `good`, it was usually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "Next, let's compute the simple accuracy score. We are hoping for accuracy that is \n",
    "better than the baseline rate. In this case, if we predicted\n",
    "all `bad`, accuracy would be around 80%! This sounds high, but it is only the baseline\n",
    "level to clear. First, use the `Counter` \n",
    "we created early (this is a\n",
    "[dictionary](https://www.w3schools.com/python/python_dictionaries.asp))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute baseline accuracy (predict all bad)\n",
    "baseline = test_labels_stats['bad'] / (test_labels_stats['good'] + test_labels_stats['bad'])\n",
    "print(\"Baseline accuracy is:\", baseline)\n",
    "\n",
    "# compute the observed accuracy\n",
    "acc = metrics.accuracy_score(test['label'], predicted)\n",
    "print(\"Observed accuracy is:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline for `bad` traffic in this dataset is just over 80%. By predicting everything is `bad`, we could achieve that level of accuracy. Of course, this would be a totally useless model. \n",
    "\n",
    "The `classification` report gives us other important staistics,\n",
    "like `precision`, `recall`, and `F1 score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = metrics.classification_report(test['label'], predicted, digits=4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If filtering out `bad` traffic was most important, we would focus on that statistic in the table above. \n",
    "It blocks over 99.9% of bad traffic (see `recall` for `bad`)! \n",
    "However, we want to allow good traffic ***and*** block bad traffic, so this statistic is not enough by itself. \n",
    "\n",
    "What is the recall for the `good` class? Would this classifier be suitable to use in a production network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Try with different predictor variables. Does the model improve?\n",
    "2. Try with different parameters for the tree. The list of adjustable parameters is\n",
    "   [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontree#sklearn.tree.DecisionTreeClassifier). \n",
    "3. (Optional): Try running the models with the full dataset. Report all statistics and your interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
