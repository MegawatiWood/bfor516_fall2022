{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Topic Modeling\n",
    "\n",
    "In this lab, we will try to predict fraudulent statements from the CEO/CFO of Northwest Pipe Co., a company that overstated revenues\n",
    "for several years before being charged with securities fraud ([Story](https://www.columbian.com/news/2011/jan/04/northwest-pipe-faces-new-allegations/)).\n",
    "\n",
    "The data was originally used in [this study](https://journals.sagepub.com/doi/abs/10.1177/0261927X15586792). It contains each sentence\n",
    "from several earnings calls that took place during the period of fraud. Each sentence has been labeled as relevant to the fraud,\n",
    "i.e. something that was later restated in revised financial statements. \n",
    "\n",
    "The original study concluded that the executives used different linguistic and verbal patterns when they were discussing fraud-related items.\n",
    "However, the study did not look at the topics of each sentence. This type of analysis could lead to greater insight into how \n",
    "fraudelent financial earnings could be presented to analysts and investors. \n",
    "\n",
    "Today, we will use Latent Dirichlet Allocation (LDA) to summarize the topics of the text. LDA is the most popular method for\n",
    "identifying the topics in a corpus of text. \n",
    "\n",
    "## Before you begin\n",
    "To conduct LDA analysis, we will use the `gensim` library in Python. This does not come with Anaconda by default, so we will need\n",
    "to download it. You can use the Anaconda Navigator. If you prefer the command line (faster method), you can follow \n",
    "[these instructions](https://docs.anaconda.com/anaconda/user-guide/getting-started/#open-anaconda-prompt).\n",
    "\n",
    "To install `gensim` in the Anaconda Prompt, just type `conda install gensim` and confirm the install when it asks to do so.\n",
    "\n",
    "# Data Exploration\n",
    "The data is on Blackboard. Import the data and run a few summary statistics. The\n",
    "primary column of interest is `Restatement Topic`, which indicates that the sentence\n",
    "was related to the fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment import vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_data = pd.read_csv('data/earningscall_fraud.csv')\n",
    "\n",
    "print(call_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of fraud\n",
    "print(call_data['Restatement Topic'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there more restatement topic sentences in the presentation or Q&A session?\n",
    "print(call_data.groupby('PRES')['Restatement Topic'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the text\n",
    "Like the previous lab, we will need to clean the raw text before fitting the models. \n",
    "The `preprocess_string` function performs many common tasks for us, including converting to lowercase,\n",
    "removing punctuation, removing stopwords, and [stemming](https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_stemming_lemmatization.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim import corpora\n",
    "\n",
    "call_data['clean_text'] = call_data['Sentence'].apply(preprocess_string)\n",
    "print(call_data.loc[1, ['Sentence', 'clean_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a dictionary with all of the words in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(call_data['clean_text'])\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words\n",
    "Now, we can create a word count matrix. This has columns for \n",
    "each word and rows represent a document (a sentence, in this case).\n",
    "The entries in the matrix are the counts of a word in a document. \n",
    "They are indexed numerically, so looking at this list will not be \n",
    "of human use without later re-linking.\n",
    "\n",
    "BOW is an acronym for bag-of-words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in call_data['clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting an LDA model\n",
    "Next, we will fit the model. One important consideration with LDA is that you must \n",
    "choose the number of topics in advance. The total number of topics allowed is not \n",
    "restricted, but too few topics and they will be too general interpret, too many topics \n",
    "and there may be considerable overlap. Later, we will see how to measure fit for different\n",
    "topic counts. This dataset is small, so model fitting is fast. Larger datasets could\n",
    "take minutes, hours, or days.\n",
    "\n",
    "For this first example, let's try with 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "lda_10 = models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing topics\n",
    "First, let's view the topics the model determined. The topics show the top words associated with each topic, by probability. \n",
    "In practice, we can use this to summarize or label what each topic corresponds to. Often, this has some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in lda_10.show_topics():\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics per sentence\n",
    "After we fit the model, we can see the probabilities for each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in bow_corpus[0:9]:\n",
    "    print(lda_10.get_document_topics(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:\n",
    "To evaluate topic model fit, we can use perplexity or coherence. These measures indicate improvement as they get \n",
    "closer to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perplexity: ', lda_10.log_perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_10, texts=call_data['clean_text'], dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove additional stopwords \n",
    "\n",
    "In the model above, some words, like *million, quarter,* and *year*, appear as the highest probability\n",
    "words for each topic. They add very little to our ability to interpret the topics generated by this model. \n",
    "These types of words are called `stopwords`, and it is commom practice in LDA to remove these words. \n",
    "\n",
    "We will import a pre-defined list of stopwords that is based on financial reports to get rid of these words.\n",
    "Then, we will define a function re-process the sentences and remove these additional stopwords (many of \n",
    "which were not included in the original stopword removal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('data/stoplist.txt', 'r') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" preprocess string and remove words from custom stopword list. \"\"\"\n",
    "    result = []\n",
    "\n",
    "    for word in preprocess_string(text):\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "call_data['clean_newstop'] = call_data['Sentence'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new dictionary and corpus based on the new results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dictionary = corpora.Dictionary(call_data['clean_newstop'])\n",
    "print(new_dictionary)\n",
    "\n",
    "new_corpus = [new_dictionary.doc2bow(text) for text in call_data['clean_newstop']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_new = models.LdaModel(new_corpus, num_topics=10, id2word=new_dictionary)\n",
    "\n",
    "for topic in lda_new.show_topics():\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new topic list is much better, though still not great. Perhaps adjusting the number of topics could \n",
    "be beneficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in new_corpus[0:9]:\n",
    "    print(lda_new.get_document_topics(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perplexity: ', lda_new.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perplexity: ', lda_new.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with topic models\n",
    "We can use the results of this analysis to build a classification model. The features used\n",
    "to predict fraud will consist of the topic probability for each sentence. For example,\n",
    "a sentence might have a 10% probability of belonging to Topic 0, a 3.1% chance of belonging to Topic 1, etc.\n",
    "The next step will calculate the topic probabilities for each document and add them to our original data frame.\n",
    "The columns with numerical names represent the topic probability for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "all_topics = lda_new.get_document_topics(new_corpus, minimum_probability=0.0)\n",
    "all_topics_csr = corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "\n",
    "# make topic names easier to read\n",
    "topic_names = ['Topic ' + str(x) for x in all_topics_df.columns]\n",
    "all_topics_df.columns = topic_names\n",
    "\n",
    "\n",
    "classification_df = pd.concat([call_data, all_topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier\n",
    "We can now build a classifier to see how well the topics perform at predicting whether a sentence is \n",
    "related to the fraud or not. \n",
    "\n",
    "Since this data is small, we will use 5 fold cross validation to estimate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "pred_vars = topic_names + ['PRES', 'WORDCOUNT', 'CEO', 'TURN_AT_TALK']\n",
    "\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "nb = GaussianNB()\n",
    "\n",
    "classifiers = [rf, nb]\n",
    "\n",
    "for clf in classifiers:\n",
    "    cv_clf = cross_validate(clf, classification_df[pred_vars], classification_df['Restatement Topic'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "    # print(cv_clf)\n",
    "    print('------', clf.__class__.__name__, '------')\n",
    "    print(\"Mean Accuracy:\", cv_clf['test_accuracy'].mean())\n",
    "    print(\"Mean F1:\", cv_clf['test_f1'].mean())\n",
    "    print(\"Mean ROC:\", cv_clf['test_roc_auc'].mean())\n",
    "    print(\"Mean Log Loss:\", cv_clf['test_neg_log_loss'].mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Try fitting LDA with just 5 topics instead of 10. How does this affect human interpretability, perplexity, coherence, and classification performance?\n",
    "2. Try fitting LDA with 15 topics. How does this affect human interpretability, perplexity, coherence, and classification performance?\n",
    "3. In addition to Random Forest, try another classifier of your choosing. How does this compare to the Random Forest?\n",
    "\n",
    "## Optional Exercises \n",
    "1. Run sentiment analysis on this data. Does adding that to a classifier improve performance?\n",
    "2. See the section below on weighted word counts. Does using tf-idf improve human interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weighted word counts\n",
    "The models above used actual word counts. We can also weight word counts by how many documents (sentences) they appear in.\n",
    "This gives rare words a higher weight per appearance, and common words very little weight.\n",
    "The most common method for this is term frequency-inverse document frequency (tf-idf). \n",
    "You can see a description of this method on [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(new_corpus)\n",
    "tfidf_corpus = tfidf[new_corpus]\n",
    "\n",
    "lda_tfidf = models.LdaModel(tfidf_corpus, num_topics=10, id2word=new_dictionary)\n",
    "for topic in lda_tfidf.show_topics():\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perplexity: ', lda_tfidf.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa3e7337b8d36894b7a51014394139a0eba3728352dce4cced1005d81d5028b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
