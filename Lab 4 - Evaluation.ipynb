{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Additional Evaluation Techniques\n",
    "\n",
    "# Overview\n",
    "In this lab, we will try to predict fraudulent credit card\n",
    "transactions. This is a difficult task, with most transactions\n",
    "being legitimate. \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import tree\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data comes from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
    "The data dimensions are anonymized to protect the identity of the individuals.\n",
    "The process to create the dimensions is \n",
    "[Principal Components Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccfraud = pd.read_csv('data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is highly unbalanced, with a fraction of a percent of transactions\n",
    "actually being fraudulent. Fraudulent transactions are about \\$34 larger than legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccfraud.describe()\n",
    "# get the average and count for each type\n",
    "ccstats = ccfraud.groupby('Class')['Amount'].agg(['mean', 'count'])\n",
    "# stats for fraud by count and average transaction amount\n",
    "print(ccstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of fraudulent transactions\n",
    "print(\"Fraudulent transaction ratio:\", ccstats.loc[1, 'count']/ccstats['count'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "This dataset is already very clean, and does not need any preprocessing.\n",
    "This is a somewhat large dataset, consisting of 284k rows. You may wish\n",
    "to use a subset of about 10,000 rows from the data while writing your code to speed up\n",
    "model training. Once your code is proper, you can remove the\n",
    "subset and instead use the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing sets\n",
    "One recommendation is to use `np.random.seed()` to make reproducible results.\n",
    "The random number generator will always produce the same \"random\" numbers\n",
    "when a specific value is set for `np.random.seed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create train and test sets of the data. We will\n",
    "fit the model with the training set, and use the test set to evaluate the model.\n",
    "We will do a 75/25 split (75% will be training data).\n",
    "Like the prior lab, use the built in methods for sampling in the `train_test_split()` function.\n",
    "\n",
    "Now, we can use these same functions and principles to generate a random\n",
    "sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ccfraud, test_size=0.25)\n",
    "print(\"Rows in train:\", len(train))\n",
    "print(\"Rows in test:\", len(test))\n",
    "train_stats = train.groupby('Class')['Amount'].agg(['mean', 'count'])\n",
    "print(\"Training data:\\n\", train_stats)\n",
    "test_stats = test.groupby('Class')['Amount'].agg(['mean', 'count'])\n",
    "print(\"Testing data:\\n\", test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "We will again use decision trees to create the model. \n",
    "\n",
    "First, let's look at using column names, rather than column indices,\n",
    "to select variables in the model. You can specify the values\n",
    "in a list, and then  `.loc` locate the columns by name in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all columns\n",
    "print(list(ccfraud.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use column names \n",
    "pred_vars = ['Time', 'Amount', 'V8', 'V1']\n",
    "print(ccfraud.loc[:, pred_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's train the actual model. We will use the columns specified in `pred_vars`. An advantage \n",
    "of using column names in this way is that we only need to update `pred_vars` in one location, and \n",
    "then reuse that list in all other instances where we train or test the model. In this case, I have\n",
    "selected `\"entropy\"` as the measure to compute the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dtree.fit(train.loc[:, pred_vars], train['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some statistics about the decision tree that we can view. \n",
    "First, let's see how many leaves, or splits, there are in this tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtree.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large tree! It may have over-fit, but we won't know until evaluating with our holdout data.\n",
    "We can also look at tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtree.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance\n",
    "\n",
    "Now, we will evaluate our model with previously unseen data \n",
    "(the test set). This will give us a vector of predicted labels.\n",
    "There are two prediction functions for decision trees. First,\n",
    "we will use the `predict()` function to see the class lable (0 or 1).\n",
    "you to get predicted class probabilities (not shown in this lab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = dtree.predict(test.loc[:, pred_vars])\n",
    "pred_labels[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how well the model performed on the new data.\n",
    "First, let's view the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(dtree, test.loc[:, pred_vars], test['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the classification report for the various\n",
    "measures of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test['Class'], pred_labels, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Evaluation\n",
    "The probabilistic predictions are useful for applications and advanced\n",
    "evaluation metrics. First, we need to get probabilities for \n",
    "each class (rather than a fixed label). The documentation\n",
    "for this is \n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba).\n",
    "The result is a matrix with two columns, one for each class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = dtree.predict_proba(test.loc[:, pred_vars])\n",
    "pred_probs[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows show predictions with absolute confidence (100% not fraud).\n",
    "This is due to the way that decision trees compute probabilities. If a branch\n",
    "of the tree is pure, i.e. every observation on the branch is of the same class,\n",
    "then any observation that reaches that endpoint will have a probability of 100%.\n",
    "\n",
    "This looks like a classic case of overfitting. The tree is extremely deep,\n",
    "having 39 layers and over 500 nodes with just four predictor variables.\n",
    "\n",
    "These extreme probability values are likely to be punished by model \n",
    "evaluation statistics that rely on probability rather than class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the Curve (AUC)\n",
    "AUC is a measure that compares the tradeoff between the true positive rate and the true negative rate.\n",
    "This measure is not affected by imbalanced classes, and it can be displayed on a graph.\n",
    "The baseline for this measure is 0.5 (random guessing). \n",
    "\n",
    "First, let's compute the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(test['Class'], pred_probs[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the curve. Since there is very little variation in probabilities \n",
    "for the predictions, this curve will not look like much of a curve. The \n",
    "exercises later will show different results. The plot function\n",
    "also displays the AUC on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_roc_curve(dtree, test.loc[:, pred_vars], test['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run similar stats to AUC, but instead of TPR and FPR,\n",
    "we can use precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.average_precision_score(test['Class'], pred_probs[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the precision/recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_precision_recall_curve(dtree, test.loc[:, pred_vars], test['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss\n",
    "[Log loss](http://wiki.fast.ai/index.php/Log_Loss) is a measure that considers\n",
    "the confidence of a prediction, rather than just the correctness. The lower \n",
    "this number, the better the model is. It is not necessarily useful on its own, like\n",
    "precision, recall, or $F_1$, \n",
    "but it is a very straightforward way to compare models. Values of log loss can be as\n",
    "low as 0 and infinitely high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.log_loss(test['Class'], pred_probs[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the value here as a baseline for additional iterations of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. What are the strengths and weaknesses of each evaluation criteria (precision/recall/F1/accuracy; model cost; log loss)?\n",
    "2. This model is severly over-fit. Try creating a new model and \n",
    "    restricting the maximum depth of the tree to 5 levels (using the `max_depth` parameter).\n",
    "    Run the various evaluation statistics on this new model.\n",
    "    1. How does the tree compare to the original model? \n",
    "    2. On which measures is it better/worse?\n",
    "2. Does adding additional variables to the model improve performance?\n",
    "3. This data is anonymized, which means the column names and their values have been\n",
    "    obscured. What data columns do you think would be useful for detecting fraudulent\n",
    "    credit card transactions?\n",
    "    \n",
    "## Optional Exercises\n",
    "1. What happens when you build the model using a different value for `np.random.seed()`? \n",
    "2. In this context, which evaluation statistic do you think is most relevant? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "There are a few other elements of decision trees that I would like to mention here.\n",
    "First, we can see the tree by plotting. This works well on small trees, but for\n",
    "larger trees like those in this lab, the value is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tree.plot_tree(dtree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting part of decision trees is viewing the path that a row\n",
    "takes to reach the decision. Again, this is typically more useful for smaller trees.\n",
    "\n",
    "To interpret this, look at the first value in parentheses. It corresponds to the row\n",
    "in the dataset (in this case, we are looking at the subset of fraudulent observations only).\n",
    "The second value in parentheses is the leaf, and the final value is the brach that the \n",
    "row took through the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_for_fraud = dtree.decision_path(train.loc[train['Class'] == 1, pred_vars])\n",
    "print(paths_for_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
